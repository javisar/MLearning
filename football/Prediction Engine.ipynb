{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://football-data.co.uk/data.php\n",
    "https://github.com/llSourcell/Predicting_Winning_Teams\n",
    "https://github.com/RudrakshTuwani/Football-Data-Analysis-and-Prediction\n",
    "https://www.kaggle.com/cactusplants/if-football-players-were-betting-on-themselves\n",
    "https://thuijskens.github.io/2016/12/29/bayesian-optimisation/\n",
    "https://datascience.stackexchange.com/questions/9488/xgboost-give-more-importance-to-recent-samples\n",
    "https://github.com/sachinruk/deepschool.io\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "https://stackoverflow.com/questions/43818584/custom-loss-function-in-keras\n",
    "https://github.com/fchollet/keras/issues/7275\n",
    "https://neat-python.readthedocs.io/en/latest/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import scipy as scipy\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as il\n",
    "import nbimporter\n",
    "\n",
    "import data_functions as df\n",
    "il.reload(df)\n",
    "import model_functions as mf\n",
    "il.reload(mf)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_season = 11    #0...16\n",
    "num_seasons = 6    #0...16\n",
    "\n",
    "\n",
    "drop_columns = ['Unnamed: 0','HomeTeam', 'AwayTeam', 'Date',  'HTFormPtsStr', 'ATFormPtsStr', 'FTHG', 'FTAG',\n",
    "           'HomeTeamLP', 'AwayTeamLP','HTFormPts','ATFormPts',\n",
    "           'HTLossStreak5','ATLossStreak5','HTWinStreak5','ATWinStreak5',\n",
    "           'HTWinStreak3','HTLossStreak3','ATWinStreak3','ATLossStreak3',\n",
    "                'HM4','HM5','AM4','AM5',\n",
    "                #'MW',\n",
    "                #'DiffPts',\n",
    "                'HTGS', 'ATGS', 'HTGC', 'ATGC',\n",
    "               ]\n",
    "\n",
    "cols_to_scale = ['HTGD','ATGD','HTP','ATP','DiffLP']\n",
    "\n",
    "data = pd.read_csv('./Datasets/final_dataset.csv')\n",
    "data = df.get_seasons(data, start_season, num_seasons)\n",
    "data = df.delete_first_3_weeks(data)\n",
    "#data = data[350*s_num_temporadas:len(data)-350*num_temporadas]\n",
    "#display(data.tail())\n",
    "data = df.drop_basic_columns(data, drop_columns)\n",
    "#data = df.drop_teams_onehot(data)\n",
    "#data = df.odds_to_prob(data)\n",
    "#data = df.explore_data(data)\n",
    "#data = df.scatter(data)\n",
    "#data = df.extract_pca(data)\n",
    "data, scaler = df.scale_features(data,cols_to_scale)\n",
    "#data = df.fill_nan(data)\n",
    "#data = data.dropna()\n",
    "data = df.form_to_str(data)\n",
    "data = df.preprocess_features(data)\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "#print(\"\\nFeature values:\")\n",
    "#display(data.head())\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Separate into feature set and target variable\n",
    "X_all = data.drop(['FTR'],1)\n",
    "#y_all = df.binarize_FTR(data['FTR'],label='H')\n",
    "\n",
    "\n",
    "clf_H = mf.train_label_xgb(X_all,df.binarize_FTR(data['FTR'],label='H'),'H',1)\n",
    "display(clf_H)\n",
    "#clf_D = mf.train_label_xgb(X_all,df.binarize_FTR(data['FTR'],label='D'),'D',0)\n",
    "#display(clf_D)\n",
    "#clf_A = mf.train_label_xgb(X_all,df.binarize_FTR(data['FTR'],label='A'),'A',0)\n",
    "#display(clf_A)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ddata = data.dropna()\n",
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "clf_H_svc = mf.train_label_svc(X_all,df.binarize_FTR(data['FTR'],label='H'),'H')\n",
    "display(clf_H_svc)\n",
    "#clf_D_svc = train_label_svc(data,'D')\n",
    "#clf_A_svc = train_label_svc(data,'A')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ddata = data.dropna()\n",
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "clf_H_lg = mf.train_label_lg(X_all,df.binarize_FTR(data['FTR'],label='H'),'H')\n",
    "display(clf_H_lg)\n",
    "#clf_D_lg = train_label_lg(data,'D')\n",
    "#clf_A_lg = train_label_lg(data,'A')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ddata = data.dropna()\n",
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vclf = VotingClassifier(estimators=[\n",
    "       ('xg', clf_H), ('svc', clf_H_svc), ('lg', clf_H_lg)],\n",
    "       voting='soft',# weights=[2,1,1],\n",
    "       #flatten_transform=True\n",
    "    )\n",
    "\n",
    "vclf = mf._train_label(vclf,X_all,df.binarize_FTR(data['FTR'],label='H'),'H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the parameters of XGBoost."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_all = data.drop(['FTR'],1)\n",
    "y_all = df.binarize_FTR(data['FTR'],label='H')\n",
    "\n",
    "parameters_grid = { 'learning_rate' : [0.1,0.03,0.01],\n",
    "               'n_estimators' : [40,60,80,100,120],\n",
    "               'max_depth': [3,4,5,6,7,8],\n",
    "               #'min_child_weight': [3],\n",
    "               'gamma':[0,0.4,1],\n",
    "               #'subsample' : [0.8],\n",
    "               #'colsample_bytree' : [0.8],\n",
    "               #'scale_pos_weight' : [1],\n",
    "               #'reg_alpha':[1e-5]\n",
    "             } \n",
    "\n",
    "parameters_random = { 'learning_rate' : scipy.stats.uniform(),\n",
    "               'n_estimators' : scipy.stats.randint(1,121),\n",
    "               'max_depth': [1,2,3,4,5,6,7,8],\n",
    "               #'min_child_weight': [3],\n",
    "               'gamma':[0,0.5,1],\n",
    "               'subsample' : scipy.stats.uniform(),\n",
    "               'colsample_bytree' : scipy.stats.uniform(),\n",
    "               #'scale_pos_weight' : [1],\n",
    "               #'reg_alpha':[1e-5]\n",
    "             }  \n",
    "\n",
    "grid_obj = mf.tune_parameters(X_all, y_all, 'H', parameters_grid, random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fitting the model on the whole dataset for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "parameters_xg = { 'learning_rate' : [0.03],\n",
    "               'n_estimators' : [100],\n",
    "               'max_depth': [8],   #6\n",
    "               'min_child_weight': [5],\n",
    "               'gamma':[0.2],\n",
    "               'subsample':[0.8],\n",
    "               'colsample_bytree':[0.8],\n",
    "               'scale_pos_weight' : [1],\n",
    "               'reg_alpha':[1e-2]\n",
    "             } \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#clf_H = mf.get_model(xgb.XGBClassifier(seed=2),X_all,df.binarize_FTR(data['FTR'],label='H'),parameters_xg,'H',1)\n",
    "clf_H = mf.get_model(xgb.XGBClassifier(seed=2),\n",
    "                     X_all,\n",
    "                     df.binarize_FTR(data['FTR'],label='H'),\n",
    "                     parameters_xg,'H',1)\n",
    "\n",
    "#clf_D = mf.get_model(xgb.XGBClassifier(seed=2),\n",
    "#                     X_all,\n",
    "#                     df.binarize_FTR(data['FTR'],label='D'),\n",
    "#                     parameters_xg,'D',0)\n",
    "#\n",
    "#clf_A = mf.get_model(xgb.XGBClassifier(seed=2),\n",
    "#                     X_all,\n",
    "#                     df.binarize_FTR(data['FTR'],label='A'),\n",
    "#                     parameters_xg,'A',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "weights=[]\n",
    "for idx in range(len(X_all)):\n",
    "    #mod = ((int(idx / 350)+1)^2)/100#*0.05\n",
    "    mod = (int(idx / 350)+1)#*0.05\n",
    "    weights.append(mod)\n",
    "\n",
    "clf_H = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0.2, learning_rate=0.03, max_delta_step=0, max_depth=8,\n",
    "       min_child_weight=5, missing=None, n_estimators=100, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=2, silent=True, subsample=0.8)\n",
    "\n",
    "clf_H.fit(X_all, df.binarize_FTR(data['FTR'],label='H'), sample_weight=weights)\n",
    "display(clf_H)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "#f1, acc = predict_labels(clf, X_train, y_train, label_to_train)\n",
    "f1, acc = mf.predict_labels(clf_H, X_all, df.binarize_FTR(data['FTR'],label='H'), 'H')\n",
    "display(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = df.binarize_FTR(data['FTR'],label='D').value_counts()['N']/df.binarize_FTR(data['FTR'],label='D').value_counts()['D']\n",
    "    \n",
    "clf_D = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0.2, learning_rate=0.03, max_delta_step=0, max_depth=8,\n",
    "       min_child_weight=5, missing=None, n_estimators=100, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
    "       scale_pos_weight=weight, seed=2, silent=True, subsample=0.8)\n",
    "\n",
    "clf_D.fit(X_all, df.binarize_FTR(data['FTR'],label='D'), sample_weight=weights)\n",
    "display(clf_D)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "#f1, acc = predict_labels(clf, X_train, y_train, label_to_train)\n",
    "f1, acc = mf.predict_labels(clf_D, X_all, df.binarize_FTR(data['FTR'],label='D'), 'D')\n",
    "display(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = df.binarize_FTR(data['FTR'],label='A').value_counts()['N']/df.binarize_FTR(data['FTR'],label='A').value_counts()['A']\n",
    "\n",
    "clf_A = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0.2, learning_rate=0.03, max_delta_step=0, max_depth=8,\n",
    "       min_child_weight=5, missing=None, n_estimators=100, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
    "       scale_pos_weight=weight, seed=2, silent=True, subsample=0.8)\n",
    "\n",
    "clf_A.fit(X_all, df.binarize_FTR(data['FTR'],label='A'), sample_weight=weights)\n",
    "display(clf_A)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "#f1, acc = predict_labels(clf, X_train, y_train, label_to_train)\n",
    "f1, acc = mf.predict_labels(clf_A, X_all, df.binarize_FTR(data['FTR'],label='A'), 'A')\n",
    "display(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "parameters_svc = { 'C':[1.0],\n",
    "                  'cache_size':[200],\n",
    "                  'class_weight':[None],\n",
    "                  'coef0':[0.0],\n",
    "                  'decision_function_shape':[None],\n",
    "                  'degree':[3], \n",
    "                  'gamma':['auto'],\n",
    "                  'kernel':['rbf'],\n",
    "                  'max_iter':[-1], \n",
    "                  'probability':[True],\n",
    "                  'shrinking':[True],\n",
    "                  'tol':[0.001],\n",
    "             } \n",
    "\n",
    "clf_H_svc = mf.get_model(SVC(random_state = 2, kernel='rbf',  probability=True),\n",
    "                     X_all,df.binarize_FTR(data['FTR'],label='H'),parameters_svc,'H',1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "parameters_lg = { 'C':[1.0], \n",
    "                 'class_weight':[None],\n",
    "                 'dual':[False],\n",
    "                 'fit_intercept':[True],\n",
    "                 'intercept_scaling':[1],\n",
    "                 'max_iter':[100],\n",
    "                 'multi_class':['ovr'],\n",
    "                 'n_jobs':[1],\n",
    "                 'penalty':['l2'],\n",
    "                 'solver':['liblinear'],\n",
    "                 'tol':[0.0001],\n",
    "             } \n",
    "\n",
    "clf_H_lg = mf.get_model(LogisticRegression(random_state = 42),\n",
    "                     X_all,df.binarize_FTR(data['FTR'],label='H'),parameters_lg,'H',1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#ddata = data.dropna()\n",
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vclf = VotingClassifier(estimators=[\n",
    "       ('xg', clf_H), ('svc', clf_H_svc), ('lg', clf_H_lg)],\n",
    "       voting='hard',# weights=[2,1,1],\n",
    "       #flatten_transform=True\n",
    "    )\n",
    "\n",
    "vclf = mf._train_label(vclf,X_all,df.binarize_FTR(data['FTR'],label='H'),'H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [\n",
    "    df.binarize_FTR(data['FTR'],label='H').value_counts()['N']/df.binarize_FTR(data['FTR'],label='H').value_counts()['H'],\n",
    "    df.binarize_FTR(data['FTR'],label='D').value_counts()['N']/df.binarize_FTR(data['FTR'],label='D').value_counts()['D'],\n",
    "    df.binarize_FTR(data['FTR'],label='A').value_counts()['N']/df.binarize_FTR(data['FTR'],label='A').value_counts()['A']\n",
    "]\n",
    "\n",
    "weights=[]\n",
    "counter = 0\n",
    "for idx, row in data.iterrows():\n",
    "    #print(row['FTR'])\n",
    "    if (row['FTR'] == 'H'):         mod = weight[0]\n",
    "    elif (row['FTR'] == 'D'):        mod = weight[1]\n",
    "    elif (row['FTR'] == 'A'):        mod = weight[2]\n",
    "    #mod = ((int(idx / 350)+1)^2)/100#*0.05\n",
    "    mod *= (int(counter / 350)+1)*2*0.1#*0.05\n",
    "    #mod=1\n",
    "    counter += 1\n",
    "    \n",
    "    \n",
    "    weights.append(mod)\n",
    "#display(weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_all = data.drop(['FTR'],1)\n",
    "\n",
    "#weights=[]\n",
    "#for idx in range(len(X_all)):\n",
    "#    #mod = ((int(idx / 350)+1)^2)/100#*0.05\n",
    "#    mod = (int(idx / 350)+1)#*0.05\n",
    "#    weights.append(mod)\n",
    "\n",
    "#weight = [\n",
    "#    df.binarize_FTR(data['FTR'],label='H').value_counts()['N']/df.binarize_FTR(data['FTR'],label='H').value_counts()['H'],\n",
    "#    df.binarize_FTR(data['FTR'],label='D').value_counts()['N']/df.binarize_FTR(data['FTR'],label='D').value_counts()['D'],\n",
    "#    df.binarize_FTR(data['FTR'],label='A').value_counts()['N']/df.binarize_FTR(data['FTR'],label='A').value_counts()['A']\n",
    "#]\n",
    "\n",
    "clf_M = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0.2, learning_rate=0.03, max_delta_step=0, max_depth=8,\n",
    "       min_child_weight=5, missing=None, n_estimators=100, nthread=-1,\n",
    "       objective='multi:softmax', reg_alpha=0.01, reg_lambda=1,\n",
    "       seed=2, silent=True, subsample=0.8)\n",
    "\n",
    "#clf_M.fit(X_all, data['FTR'], sample_weight=weights)\n",
    "clf_M.fit(X_all, data['FTR'], sample_weight=weights)\n",
    "display(clf_M)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "#f1, acc = predict_labels(clf, X_train, y_train, label_to_train)\n",
    "#f1, acc = mf.predict_labels(clf_M, X_all, data['FTR'], '')\n",
    "y_pred = clf_M.predict(X_all)\n",
    "\n",
    "#f1 = f1_score(data['FTR'], y_pred, pos_label=label),\n",
    "print(confusion_matrix(data['FTR'], y_pred))\n",
    "acc = sum(data['FTR'] == y_pred) / float(len(y_pred))\n",
    "print(acc)\n",
    "#display(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.show_features_importances(clf_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "class EarlyStoppingByLossVal(keras.callbacks.Callback):\n",
    "   def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "       super(keras.callbacks.Callback, self).__init__()\n",
    "       self.monitor = monitor\n",
    "       self.value = value\n",
    "       self.verbose = verbose\n",
    "   def on_epoch_end(self, epoch, logs={}):\n",
    "       current = logs.get(self.monitor)\n",
    "       if current is None:\n",
    "           warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "       if current < self.value:\n",
    "           if self.verbose > 0:\n",
    "               print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "           self.model.stop_training = True\n",
    "        \n",
    "callbackss = [\n",
    "    #EarlyStoppingByLossVal(monitor='val_loss', value=0.16, verbose=1),\n",
    "    #keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    #ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    keras.callbacks.TensorBoard(log_dir='./tf_model_full'),\n",
    "    #ModelCheckpoint('./tf_model_full', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "X_all = data.drop(['FTR'],1)\n",
    "X_all = df.fill_nan(X_all)\n",
    "#y_all = df.binarize_FTR(data['FTR'],label='H')\n",
    "y_all = data['FTR']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_all)\n",
    "encoded_Y = encoder.transform(y_all)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, dummy_y,\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all)\n",
    "                                                    \n",
    "def create_model():\n",
    "    global data\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, activation='relu', kernel_initializer='normal', input_shape=(data.shape[1]-1,)))\n",
    "    #model.add(keras.layers.normalization.BatchNormalization())\n",
    "    #model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='normal'))\n",
    "    #model.add(keras.layers.normalization.BatchNormalization())\n",
    "    #model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(30, activation='relu', kernel_initializer='normal'))\n",
    "    #model.add(keras.layers.normalization.BatchNormalization())\n",
    "    #model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='normal'))\n",
    "    #model.add(keras.layers.normalization.BatchNormalization())\n",
    "    #model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax', kernel_initializer='normal'))\n",
    "    #model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adagrad(lr=0.1, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    display(model.summary())\n",
    "    return model\n",
    "    \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import math\n",
    "\n",
    "weight = [\n",
    "    df.binarize_FTR(data['FTR'],label='H').value_counts()['N']/df.binarize_FTR(data['FTR'],label='H').value_counts()['H'],\n",
    "    df.binarize_FTR(data['FTR'],label='D').value_counts()['N']/df.binarize_FTR(data['FTR'],label='D').value_counts()['D'],\n",
    "    df.binarize_FTR(data['FTR'],label='A').value_counts()['N']/df.binarize_FTR(data['FTR'],label='A').value_counts()['A']\n",
    "]\n",
    "\n",
    "class_weight = {0 : math.log(weight[2]),\n",
    "    1: math.log(weight[1]),\n",
    "    2: math.log(weight[0])}\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=500, batch_size=320, verbose=1)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "y_keras = dummy_y\n",
    "#y_keras = pd.DataFrame(dummy_y,index=X_all.index).join(X_all).values\n",
    "results = cross_val_score(estimator, X_all.values, y_keras, cv=kfold,  fit_params={\n",
    "        #'sample_weight': np.array(weights),\n",
    "        #'class_weight': class_weight,\n",
    "        'callbacks': callbackss\n",
    "            }\n",
    "                         )\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# KERAS CATEGORICAL\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "#import tensorflow as tf\n",
    "\n",
    "#custom_l = []\n",
    "#IWH_x = X_all.columns.get_loc('IWH')\n",
    "#IWD_x = X_all.columns.get_loc('IWD')\n",
    "#IWA_x = X_all.columns.get_loc('IWA')\n",
    "\n",
    "def customLoss(yTrue,yPred):\n",
    "    #display(yTrue)\n",
    "    #x_training = yTrue[..., 3:]\n",
    "    #IWH = yTrue[..., 20+3]\n",
    "    #IWD = yTrue[..., 21+3]\n",
    "    #IWA = yTrue[..., 22+3]\n",
    "    \n",
    "    y_true = yTrue[..., :3]    \n",
    "    y_pred = yPred[..., :3]\n",
    "    \n",
    "    #\n",
    "    #kk = y_true[...,0]*IWA\n",
    "    \n",
    "    #global custom_l\n",
    "    #custom_l.append(y_true)\n",
    "    return categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "def customMetric(yTrue,yPred):\n",
    "    y_true = yTrue[..., :3]\n",
    "    \n",
    "    y_pred = yPred[..., :3]\n",
    "    return categorical_accuracy(y_true,y_pred)\n",
    "\n",
    "def metric_balance(yTrue,yPred):\n",
    "\n",
    "    IWH = yTrue[..., 20+3]\n",
    "    IWD = yTrue[..., 21+3]\n",
    "    IWA = yTrue[..., 22+3]\n",
    "    \n",
    "    y_true = yTrue[..., :3]    \n",
    "    y_pred = yPred[..., :3]\n",
    "    \n",
    "    #\n",
    "    #k1 = y_true[...,0]*IWA\n",
    "    #k2 = y_true[...,1]*IWD\n",
    "    #k3 = y_true[...,2]*IWH\n",
    "    \n",
    "    #return K.sum(K.sum(k1,k2),k3)\n",
    "    return categorical_accuracy(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# KERAS CATEGORICAL\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_all = data.drop(['FTR'],1)\n",
    "X_all = df.fill_nan(X_all)\n",
    "#y_all = df.binarize_FTR(data['FTR'],label='H')\n",
    "y_all = data['FTR']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_all)\n",
    "encoded_Y = encoder.transform(y_all)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, dummy_y,\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all)\n",
    "\n",
    "def create_model():\n",
    "    import keras\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    global data\n",
    "    \n",
    "    main_input = keras.layers.Input(shape=(data.shape[1]-1,), name='main_input')\n",
    "    \n",
    "    #auxiliary_input = keras.layers.Input(shape=(data.shape[1]-1,), name='aux_input')\n",
    "    #x = keras.layers.concatenate([main_input, auxiliary_input])\n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    x = Dense(40, activation='relu', kernel_initializer='normal', input_shape=(data.shape[1]-1,))(main_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(20, activation='relu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(10, activation='relu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(3, activation='softmax', kernel_initializer='normal')(x)\n",
    "    x = keras.layers.concatenate([x, main_input])\n",
    "    \n",
    "    model = keras.models.Model(inputs=[main_input], outputs=[x])\n",
    "    \n",
    "    model.compile(loss=customLoss,#'categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adagrad(lr=0.1, epsilon=1e-08, decay=0.0),\n",
    "              #metrics=[customMetric,metric_balance,'accuracy']\n",
    "                metrics=[customMetric]\n",
    "                 )\n",
    "\n",
    "    display(model.summary())\n",
    "    return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import math\n",
    "\n",
    "weight = [\n",
    "    df.binarize_FTR(data['FTR'],label='H').value_counts()['N']/df.binarize_FTR(data['FTR'],label='H').value_counts()['H'],\n",
    "    df.binarize_FTR(data['FTR'],label='D').value_counts()['N']/df.binarize_FTR(data['FTR'],label='D').value_counts()['D'],\n",
    "    df.binarize_FTR(data['FTR'],label='A').value_counts()['N']/df.binarize_FTR(data['FTR'],label='A').value_counts()['A']\n",
    "]\n",
    "\n",
    "class_weight = {\n",
    "    0: math.log(weight[2]),\n",
    "    1: math.log(weight[1]),\n",
    "    2: math.log(weight[0])}\n",
    "\n",
    "dd = dict((key+3, 1) for (key) in range(114))\n",
    "dall = {}\n",
    "dall.update(class_weight)\n",
    "dall.update(dd)\n",
    "class_weight = dall\n",
    "\n",
    "########################              \n",
    "model = create_model()\n",
    "history = model.fit(X_train.values, pd.DataFrame(y_train,index=X_train.index).join(X_train).values,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    epochs=200,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test.values, pd.DataFrame(y_test,index=X_test.index).join(X_test).values),\n",
    "                    callbacks=callbackss,\n",
    "                    #class_weight=class_weight\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE LINE\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "    \n",
    "X_all = data.drop(['FTR'],1)\n",
    "X_all = df.fill_nan(X_all)\n",
    "y_all = data['FTR']\n",
    "\n",
    "\n",
    "def only_hw(string):\n",
    "    if string == 'A':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all.apply(only_hw),\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all.apply(only_hw))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', kernel_initializer='normal', input_shape=(X_train.shape[1],)))\n",
    "#model.add(keras.layers.normalization.BatchNormalization())\n",
    "#model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='normal'))\n",
    "#model.add(keras.layers.normalization.BatchNormalization())\n",
    "#model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(30, activation='relu', kernel_initializer='normal'))\n",
    "#model.add(keras.layers.normalization.BatchNormalization())\n",
    "#model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation='relu', kernel_initializer='normal'))\n",
    "#model.add(keras.layers.normalization.BatchNormalization())\n",
    "#model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "#model.add(keras.layers.advanced_activations.LeakyReLU(0.3))\n",
    "display(model.summary())\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adagrad(lr=0.1, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "class EarlyStoppingByLossVal(keras.callbacks.Callback):\n",
    "   def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "       super(keras.callbacks.Callback, self).__init__()\n",
    "       self.monitor = monitor\n",
    "       self.value = value\n",
    "       self.verbose = verbose\n",
    "   def on_epoch_end(self, epoch, logs={}):\n",
    "       current = logs.get(self.monitor)\n",
    "       if current is None:\n",
    "           warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "       if current < self.value:\n",
    "           if self.verbose > 0:\n",
    "               print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "           self.model.stop_training = True\n",
    "        \n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=0.16, verbose=1),\n",
    "    #keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    #ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    keras.callbacks.TensorBoard(log_dir='./tf_model_full'),\n",
    "    #ModelCheckpoint('./tf_model_full', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    epochs=1000,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test.values, y_test.values),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test.values, y_test.values, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = model.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#http://neat-python.readthedocs.io/en/latest/index.html\n",
    "import neat\n",
    "import visualize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import random\n",
    "random.seed(42)\n",
    "    \n",
    "X_all = data.drop(['FTR'],1)\n",
    "X_all = df.drop_teams_onehot(X_all)\n",
    "X_all = df.fill_nan(X_all)\n",
    "\n",
    "y_all = data['FTR']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_all)\n",
    "encoded_Y = encoder.transform(y_all)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# 2-input XOR inputs and expected outputs.\n",
    "#xor_inputs = [(0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)]\n",
    "#xor_outputs = [   (0.0,),     (1.0,),     (1.0,),     (0.0,)]\n",
    "\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        #genome.fitness = 4.0\n",
    "        genome.fitness = 0.0\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        for xi, xo in zip(X_all.values, dummy_y):\n",
    "            output = net.activate(xi)\n",
    "            if (output[0]>=0.5 and output[1]<0.5 and output[2]<0.5 and xo[0] == 1):\n",
    "                genome.fitness += 1\n",
    "            elif (output[0]<0.5 and output[1]>=0.5 and output[2]<0.5 and xo[1] == 1):\n",
    "                genome.fitness += 1\n",
    "            elif (output[0]<0.5 and output[1]<0.5 and output[2]>=0.5 and xo[2] == 1):\n",
    "                genome.fitness += 1\n",
    "        genome.fitness /= len(X_all.values)\n",
    "        print('.', end='')\n",
    "            #genome.fitness -= (output[0] - xo[0]) ** 2\n",
    "    #print('.')\n",
    "    \n",
    "def eval_genomes_balance(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        #genome.fitness = 4.0\n",
    "        genome.fitness = 0.0\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        for xi, xo in zip(X_all.values, dummy_y):\n",
    "            output = net.activate(xi)\n",
    "            if (output[0]>=0.5):\n",
    "                genome.fitness -= 1\n",
    "                if (xo[2] == 1.0): genome.fitness += xi[20]\n",
    "            if (output[1]>=0.5):\n",
    "                genome.fitness -= 1\n",
    "                if (xo[1] == 1.0): genome.fitness += xi[21]\n",
    "            if (output[2]>=0.5):\n",
    "                genome.fitness -= 1\n",
    "                if (xo[0] == 1.0): genome.fitness += xi[22]\n",
    "            #print(xi[20],xi[21],xi[22],xo,output,genome.fitness)        \n",
    "        #genome.fitness /= len(X_all.values)\n",
    "        print('.', end='')\n",
    "            #genome.fitness -= (output[0] - xo[0]) ** 2\n",
    "    #print('.')\n",
    "\n",
    "def run(config_file):\n",
    "    # Load configuration.\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                         config_file)\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    #p.add_reporter(neat.Checkpointer(5))\n",
    "\n",
    "    # Run for up to 300 generations.\n",
    "    #winner = p.run(eval_genomes, 300)\n",
    "    winner = p.run(eval_genomes_balance, 100)\n",
    "    #pe = neat.ThreadedEvaluator(3, eval_genome)\n",
    "    #winner = p.run(pe.evaluate, 300)\n",
    "    #pe.stop()\n",
    "\n",
    "    # Display the winning genome.\n",
    "    print('\\nBest genome:\\n{!s}'.format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    print('\\nOutput:')\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(X_all.values, dummy_y):\n",
    "        output = winner_net.activate(xi)\n",
    "        #print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "    #node_names = {-1:'A', -2: 'B', 0:'A XOR B'}\n",
    "    #visualize.draw_net(config, winner, True, node_names=node_names)\n",
    "    visualize.plot_stats(stats, ylog=False, view=True)\n",
    "    #visualize.plot_species(stats, view=True)\n",
    "\n",
    "    #p = neat.Checkpointer.restore_checkpoint('neat-checkpoint-4')\n",
    "    #p.run(eval_genomes, 10)\n",
    "    return winner_net\n",
    "\n",
    "model = run('config-feedforward')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model,'neat_balance.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict the last season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tdata = pd.read_csv('./Datasets/final_dataset.csv')\n",
    "\n",
    "tdata = df.get_seasons(tdata, start_season+num_seasons, 1)\n",
    "tdata = df.delete_first_3_weeks(tdata)\n",
    "\n",
    "pred_labels=tdata[['HomeTeam','AwayTeam','IWH','IWD','IWA']]\n",
    "#tdata = tdata[len(tdata)-350*num_temporadas:len(tdata)-350*(num_temporadas-1)]\n",
    "#display(tdata.head())\n",
    "tdata = df.drop_basic_columns(tdata, drop_columns)\n",
    "#data = df.drop_teams_onehot(data)\n",
    "#data = df.odds_to_prob(data)\n",
    "#data = df.explore_data(data)\n",
    "#data = df.scatter(data)\n",
    "#tdata = df.extract_pca(tdata)\n",
    "tdata, scaler = df.scale_features(tdata, cols_to_scale, scaler)\n",
    "#tdata = df.fill_nan(tdata)\n",
    "#tdata = tdata.dropna()\n",
    "tdata = df.form_to_str(tdata)\n",
    "tdata = df.preprocess_features(tdata)\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "#print(\"\\nFeature values:\")\n",
    "#display(data.head())\n",
    "\n",
    "\n",
    "X_last = tdata.drop(['FTR'],1)\n",
    "#y_last = df.binarize_FTR(tdata['FTR'],label='H')\n",
    "\n",
    "y_last_bets = pd.DataFrame(tdata['FTR']).join(X_last[['IWH','IWD','IWA']])\n",
    "\n",
    "#y_pred_H, y_pred_prob_H = mf.simulate_predict(clf_H, X_last, df.binarize_FTR(tdata['FTR'],label='H'), 'H')\n",
    "#y_pred_D, y_pred_prob_D = mf.simulate_predict(clf_D, X_last, df.binarize_FTR(tdata['FTR'],label='D'), 'D')\n",
    "#y_pred_A, y_pred_prob_A = mf.simulate_predict(clf_A, X_last, df.binarize_FTR(tdata['FTR'],label='A'), 'A')\n",
    "\n",
    "#lbalance_H = mf.simulate_bets(y_pred_H, y_pred_prob_H, y_last_bets, 'H','IWH',0.5, 1.15)\n",
    "#lbalance_D = mf.simulate_bets(y_pred_D, y_pred_prob_D, y_last_bets, 'D','IWD',0.3, 1.15)\n",
    "#lbalance_A = mf.simulate_bets(y_pred_A, y_pred_prob_A, y_last_bets, 'A','IWA',0.6, 1.15)\n",
    "\n",
    "pred_labels['1IWH'] = 1/pred_labels['IWH']\n",
    "pred_labels['1IWA'] = 1/pred_labels['IWD']\n",
    "pred_labels['1IWD'] = 1/pred_labels['IWA']\n",
    "\n",
    "#display(pd.DataFrame(pred_labels)      \n",
    "#      .join(pd.DataFrame(y_pred_prob_H,index=pred_labels.index,columns=['PH','NH']))\n",
    "#      .join(pd.DataFrame(y_pred_prob_D,index=pred_labels.index,columns=['PD','ND']))\n",
    "#      .join(pd.DataFrame(y_pred_prob_A,index=pred_labels.index,columns=['PA','NA']))\n",
    "#     )\n",
    "\n",
    "y_pred = clf_M.predict(X_last)\n",
    "y_pred_prob = clf_M.predict_proba(X_last)\n",
    "\n",
    "acc = sum(tdata['FTR'] == y_pred) / float(len(y_pred))\n",
    "print(acc)\n",
    "\n",
    "display(pd.DataFrame(pred_labels)      \n",
    "        .join(pd.DataFrame(y_pred,index=pred_labels.index,columns=['PFTR']))\n",
    "     )\n",
    "display(y_pred_prob)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(lbalance_H) # plotting by columns\n",
    "#plt.plot(lbalance_D) # plotting by columns\n",
    "#plt.plot(lbalance_A) # plotting by columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(y_pred_prob_A[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import math\n",
    "balance = 100\n",
    "bet = balance/20\n",
    "counter = 0\n",
    "\n",
    "wins = 0\n",
    "skipped = 0\n",
    "lbalance = []\n",
    "\n",
    "for index, row in y_last_bets.iterrows():\n",
    "    ftr = row['FTR']    \n",
    "    #odds = row[column];\n",
    "    #prediction = y_pred_[counter]\n",
    "    #prediction_prob = y_pred_prod_[counter]\n",
    "    #print(prediction_prob)\n",
    "    \n",
    "    counter = counter +1\n",
    "    #print(y_pred_prob_H[counter-1][0])\n",
    "    #if ((1/prediction_prob[0])-0.05 < row['B365H'] and prediction_prob[0] > 0.65 and prediction == 'H' ):\n",
    "    #if (prediction_prob[0] > 0.5 ):    \n",
    "    #if (prediction_prob[0] > 0.45):  \n",
    "    lbalance.append(balance)\n",
    "    #print(prediction,prediction_prob[0],row[column],label)\n",
    "    #if (((prediction_prob[0] > prob and prediction == label) or (prob < 0.5 and prediction_prob[0] < prob)) and row[column]>umbral ):    \n",
    "    if (math.isnan(row['IWH']) or (row['FTR'] != 'H' and row['FTR']!='D' and row['FTR']!='A')):\n",
    "        skipped = skipped +1\n",
    "        continue       \n",
    "    #print(y_pred[counter-1])\n",
    "    #print(y_pred_prob[counter-1])\n",
    "    \n",
    "    if (y_pred[counter-1] == 'H' and y_pred_prob[counter-1][2] > 0.34 and row['IWH']>1.15 ):    \n",
    "        balance = balance - bet\n",
    "        if (ftr == 'H'):           \n",
    "            wins = wins+1\n",
    "            balance = balance + (bet*row['IWH'])       \n",
    "    elif (y_pred[counter-1] == 'A' and y_pred_prob[counter-1][0] > 0.7 and row['IWA']>1.15 ):    \n",
    "        balance = balance - bet\n",
    "        if (ftr == 'A'):           \n",
    "            wins = wins+1\n",
    "            balance = balance + (bet*row['IWA'])\n",
    "    elif (y_pred[counter-1] == 'D' and y_pred_prob[counter-1][1] > 0.45 and row['IWD']>1.15 ):    \n",
    "        balance = balance - bet\n",
    "        if (ftr == 'D'):           \n",
    "            wins = wins+1\n",
    "            balance = balance + (bet*row['IWD'])\n",
    "    else:\n",
    "        skipped = skipped +1\n",
    "        print(\"{:.0f}\\t{}\\t[{}\\t{:.3f}\\t{:.3f}\\t{:.3f}]\\t<{:.0f}>\\t[{:.3f}\\t{:.3f}\\t{:.3f}]\\t[{:.3f}\\t{:.3f}\\t{:.3f}]\\t{:.0f} skip\"\n",
    "          .format(counter-1,ftr,y_pred[counter-1],y_pred_prob[counter-1][2],y_pred_prob[counter-1][1],y_pred_prob[counter-1][0],index,1/row['IWH'],1/row['IWD'],1/row['IWA'],row['IWH'],row['IWD'],row['IWA'],balance))\n",
    "        continue\n",
    "\n",
    "    print(\"{:.0f}\\t{}\\t[{}\\t{:.3f}\\t{:.3f}\\t{:.3f}]\\t<{:.0f}>\\t[{:.3f}\\t{:.3f}\\t{:.3f}]\\t[{:.3f}\\t{:.3f}\\t{:.3f}]\\t{:.0f} \"\n",
    "          .format(counter-1,ftr,y_pred[counter-1],y_pred_prob[counter-1][2],y_pred_prob[counter-1][1],y_pred_prob[counter-1][0],index,1/row['IWH'],1/row['IWD'],1/row['IWA'],row['IWH'],row['IWD'],row['IWA'],balance))\n",
    "    #print(counter-1,ftr,prediction,index, balance)\n",
    "    \n",
    "\n",
    "total = len(y_pred)-skipped\n",
    "if (total == 0):\n",
    "    total = 1\n",
    "print(len(y_pred)-skipped, wins)\n",
    "print(\"Balance and accuracy score for training set: {:.4f} , {:.4f}.\".format(balance , (wins/total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lbalance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import math\n",
    "balance = 100\n",
    "bet = balance/20\n",
    "counter = 0\n",
    "\n",
    "wins = 0\n",
    "skipped = 0\n",
    "lbalance = []\n",
    "\n",
    "H_accuracy = 0\n",
    "D_accuracy = 0\n",
    "A_accuracy = 0\n",
    "\n",
    "H_total = 0\n",
    "D_total = 0\n",
    "A_total = 0\n",
    "\n",
    "prediction = 'N'\n",
    "prediction_prob = 1\n",
    "\n",
    "for index, row in y_last_bets.iterrows():\n",
    "    ftr = row['FTR']\n",
    "    ftrr = row['FTR']\n",
    "    oddsH = row['IWH'];\n",
    "    oddsD = row['IWD'];\n",
    "    oddsA = row['IWA'];\n",
    "\n",
    "    prediction_H = y_pred_H[counter]\n",
    "    prediction_prob_H = y_pred_prob_H[counter]\n",
    "    prediction_D = y_pred_D[counter]\n",
    "    prediction_prob_D = y_pred_prob_D[counter]\n",
    "    prediction_A = y_pred_A[counter]\n",
    "    prediction_prob_A = y_pred_prob_A[counter]\n",
    "    \n",
    "    #print(prediction_prob)\n",
    "    counter = counter +1\n",
    "\n",
    "    #bet = balance/20\n",
    "    \n",
    "    #if ((counter-1) % 10 == 0): bet = balance/20\n",
    "    #if ((1/prediction_prob[0])-0.05 < row['B365H'] and prediction_prob[0] > 0.65 and prediction == 'H' ):\n",
    "    #if (prediction_prob[0] > 0.5 ):    \n",
    "    #if (prediction_prob[0] > 0.45):  \n",
    "    lbalance.append(balance)\n",
    "    #print(prediction,prediction_prob[0],row[column],label)\n",
    "    \n",
    "    # Las 3 primeras jornadas se han quitado ya\n",
    "    \n",
    "    \n",
    "    #if (prediction_prob_H[0] > 0.5 and prediction_H == 'H' and prediction_prob_H[0]<row['IWH']):   \n",
    "    if (math.isnan(oddsH)):\n",
    "        prediction_prob = prediction_prob_H[0]\n",
    "        prediction = prediction_H\n",
    "        skipped = skipped +1\n",
    "        print(\"{:.0f}\\t{}\\t{}\\t[{:.2f}\\t{:.2f}\\t{:.2}]\\t<{:.0f}>\\t[{:.2f}\\t{:.2f}\\t{:.2f}]\\t{:.3f}\\t{:.1f} nan\".format(counter-1,ftrr,prediction,prediction_prob_H[0],prediction_prob_D[0],prediction_prob_A[0],index,oddsH,oddsD,oddsA,1/prediction_prob, balance))\n",
    "        continue\n",
    "    elif (prediction_prob_H[0] > 0.5 and prediction_H == 'H' and row['IWH']>1.15):   \n",
    "    #if (prediction_prob_H[0] > 0.5):  \n",
    "        prediction = 'H'\n",
    "        prediction_prob = prediction_prob_H[0]\n",
    "        balance = balance - bet\n",
    "        H_total = H_total +1\n",
    "        if (ftr == 'H'):\n",
    "            wins = wins+1\n",
    "            H_accuracy = H_accuracy +1\n",
    "            balance = balance + (bet*(row['IWH']))  \n",
    "    else:\n",
    "        prediction_prob = prediction_prob_H[0]\n",
    "        prediction = prediction_H\n",
    "        skipped = skipped +1\n",
    "        print(\"{:.0f}\\t{}\\t{}\\t[{:.2f}\\t{:.2f}\\t{:.2}]\\t<{:.0f}>\\t[{:.2f}\\t{:.2f}\\t{:.2f}]\\t{:.3f}\\t{:.1f} skip\".format(counter-1,ftrr,prediction,prediction_prob_H[0],prediction_prob_D[0],prediction_prob_A[0],index,oddsH,oddsD,oddsA,1/prediction_prob, balance))\n",
    "        continue\n",
    "    \n",
    "    '''\n",
    "    elif (prediction_prob_D[0] > prediction_prob_A[0]):\n",
    "        prediction = 'D'\n",
    "        prediction_prob = prediction_prob_D[0]\n",
    "        balance = balance - bet\n",
    "        if (ftr == 'D'):           \n",
    "            wins = wins+1\n",
    "            balance = balance + (bet*row['B365D'])\n",
    "    '''\n",
    "\n",
    "\n",
    "    #print(\"{:.0f}\\t{}\\t{}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t<{:.0f}>\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.1f} \".format(counter-1,ftrr,prediction,prediction_prob_H[0],prediction_prob_D[0],prediction_prob_A[0],index,oddsH,oddsD,oddsA, 1/prediction_prob,balance))\n",
    "    print(\"{:.0f}\\t{}\\t{}\\t[{:.2f}\\t{:.2f}\\t{:.2}]\\t<{:.0f}>\\t[{:.2f}\\t{:.2f}\\t{:.2f}]\\t{:.3f}\\t{:.1f}\".format(counter-1,ftrr,prediction,prediction_prob_H[0],prediction_prob_D[0],prediction_prob_A[0],index,oddsH,oddsD,oddsA,1/prediction_prob, balance))\n",
    "    #print(counter-1,ftr,prediction,index, balance)\n",
    "\n",
    "\n",
    "total = len(y_pred_H)-skipped\n",
    "if (total == 0):\n",
    "    total = 1\n",
    "if H_total == 0: H_total = 1\n",
    "if D_total == 0: D_total = 1\n",
    "if A_total == 0: A_total = 1\n",
    "    \n",
    "print(len(y_pred_H)-skipped, wins)\n",
    "print(\"Balance and accuracy score for training set: {:.4f} , {:.4f}.\".format(balance , (wins/total)))\n",
    "print(\"{:.2f}\\t{:.2f}\\t{:.2f}\".format(H_accuracy/H_total , D_accuracy/D_total, A_accuracy/A_total))\n",
    "print(\"{:.0f}\\t{:.0f}\\t{:.0f}\".format(H_total , D_total, A_total))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(lbalance) # plotting by columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
